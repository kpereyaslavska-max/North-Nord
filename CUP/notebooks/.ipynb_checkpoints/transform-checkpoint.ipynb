{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "887d8948-1bb6-4048-bbf9-cf7e32320429",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a profile of the results sheets\n",
    "\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "\n",
    "# Define path to CSV files\n",
    "csv_dir = Path('../source/csv_results')\n",
    "\n",
    "# Dictionary to store column names for each file\n",
    "column_dict = {}\n",
    "\n",
    "# Read column names from each CSV\n",
    "for csv_file in sorted(csv_dir.glob('*.csv')):\n",
    "    df = pd.read_csv(csv_file, nrows=0)  # Read only headers\n",
    "    column_dict[csv_file.name] = list(df.columns)\n",
    "    print(f\"{csv_file.name}: {len(df.columns)} columns\")\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"Total CSV files analyzed: {len(column_dict)}\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "# Check if all column sets are identical\n",
    "all_columns = list(column_dict.values())\n",
    "first_columns = all_columns[0]\n",
    "all_identical = all(cols == first_columns for cols in all_columns)\n",
    "\n",
    "if all_identical:\n",
    "    print(\"\\n✅ All CSV files have IDENTICAL column names!\")\n",
    "    print(f\"\\nColumn names ({len(first_columns)} columns):\")\n",
    "    for i, col in enumerate(first_columns, 1):\n",
    "        print(f\"  {i}. {col}\")\n",
    "else:\n",
    "    print(\"\\n⚠️  CSV files have DIFFERENT column names!\")\n",
    "    \n",
    "    # Find unique column sets\n",
    "    unique_column_sets = {}\n",
    "    for filename, cols in column_dict.items():\n",
    "        cols_tuple = tuple(cols)\n",
    "        if cols_tuple not in unique_column_sets:\n",
    "            unique_column_sets[cols_tuple] = []\n",
    "        unique_column_sets[cols_tuple].append(filename)\n",
    "    \n",
    "    print(f\"\\nFound {len(unique_column_sets)} different column structures:\\n\")\n",
    "    \n",
    "    for i, (cols, files) in enumerate(unique_column_sets.items(), 1):\n",
    "        print(f\"\\n--- Structure {i} ({len(cols)} columns) ---\")\n",
    "        print(f\"Files with this structure: {', '.join(files)}\")\n",
    "        print(f\"\\nColumns:\")\n",
    "        for j, col in enumerate(cols, 1):\n",
    "            print(f\"  {j}. {col}\")\n",
    "    \n",
    "    # Show differences between structures\n",
    "    if len(unique_column_sets) == 2:\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(\"DIFF ANALYSIS\")\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "        cols_list = list(unique_column_sets.keys())\n",
    "        set1, set2 = set(cols_list[0]), set(cols_list[1])\n",
    "        \n",
    "        only_in_first = set1 - set2\n",
    "        only_in_second = set2 - set1\n",
    "        common = set1 & set2\n",
    "        \n",
    "        print(f\"\\nCommon columns: {len(common)}\")\n",
    "        print(f\"Only in structure 1: {len(only_in_first)}\")\n",
    "        if only_in_first:\n",
    "            for col in only_in_first:\n",
    "                print(f\"  - {col}\")\n",
    "        \n",
    "        print(f\"\\nOnly in structure 2: {len(only_in_second)}\")\n",
    "        if only_in_second:\n",
    "            for col in only_in_second:\n",
    "                print(f\"  - {col}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c01b5b4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine original results sheets into a single master sheet\n",
    "\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "\n",
    "# Define columns to extract\n",
    "columns_to_extract = [\n",
    "    'ID',\n",
    "    'TITLE_AUTHOR_DATE_COMBINED_NORMALIZED',\n",
    "    'TITLE',\n",
    "    'TITLE_REMAINDER',\n",
    "    'AUTHOR',\n",
    "    'AUTHOR_QUALIFIER',\n",
    "    'AUTHOR_DATE',\n",
    "    'EDITION',\n",
    "    'BEGIN_PUBLICATION_DATE',\n",
    "    'PUBDATE_260',\n",
    "    'PUBDATE_264',\n",
    "    'PUBPLACE_260',\n",
    "    'PUBPLACE_264',\n",
    "    'PUBLISHER_260',\n",
    "    'PUBLISHER_264',\n",
    "    'EXTENT',\n",
    "    'ASSOCIATED_ISBNS',\n",
    "    'ASSOCIATED_OCLC_NUMBERS',\n",
    "    'LCCN',\n",
    "    'TITLE_MATCH_COUNT',\n",
    "    'RETENTIONS_APPLIED'\n",
    "]\n",
    "\n",
    "# Define path to CSV files\n",
    "csv_dir = Path('../source/csv_results')\n",
    "\n",
    "# List to store dataframes\n",
    "dfs_to_combine = []\n",
    "\n",
    "print(f\"Processing {len(list(csv_dir.glob('*.csv')))} CSV files...\\n\")\n",
    "\n",
    "# Process each CSV file\n",
    "for csv_file in sorted(csv_dir.glob('*.csv')):\n",
    "    print(f\"Processing: {csv_file.name}\")\n",
    "    \n",
    "    # Read the CSV\n",
    "    df = pd.read_csv(csv_file)\n",
    "    \n",
    "    # Get available columns\n",
    "    available_columns = df.columns.tolist()\n",
    "    \n",
    "    # Check which columns exist and which don't\n",
    "    missing_columns = []\n",
    "    present_columns = []\n",
    "    \n",
    "    for col in columns_to_extract:\n",
    "        if col in available_columns:\n",
    "            present_columns.append(col)\n",
    "        else:\n",
    "            missing_columns.append(col)\n",
    "            warnings.warn(f\"Column '{col}' not found in {csv_file.name}\")\n",
    "    \n",
    "    # Extract only the columns that exist\n",
    "    df_subset = df[present_columns].copy()\n",
    "    \n",
    "    # Add missing columns as NaN\n",
    "    for col in missing_columns:\n",
    "        df_subset[col] = None\n",
    "    \n",
    "    # Reorder columns to match the original list\n",
    "    df_subset = df_subset[columns_to_extract]\n",
    "    \n",
    "    # Add source column (extract meaningful name from filename)\n",
    "    source_name = csv_file.stem  # Remove .csv extension\n",
    "    df_subset.insert(0, 'SOURCE_DATASET', source_name)\n",
    "    \n",
    "    print(f\"  ✓ Extracted {len(df_subset)} rows, {len(present_columns)}/{len(columns_to_extract)} columns present\")\n",
    "    if missing_columns:\n",
    "        print(f\"  ⚠️  Missing columns: {', '.join(missing_columns)}\")\n",
    "    \n",
    "    dfs_to_combine.append(df_subset)\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"Combining all datasets...\")\n",
    "\n",
    "# Combine all dataframes vertically\n",
    "combined_df = pd.concat(dfs_to_combine, axis=0, ignore_index=True)\n",
    "\n",
    "print(f\"✓ Combined dataset shape: {combined_df.shape[0]:,} rows × {combined_df.shape[1]} columns\")\n",
    "\n",
    "# Save to CSV (disabled unless required)\n",
    "# output_path = Path('../source/combined_results.csv')\n",
    "# combined_df.to_csv(output_path, index=False)\n",
    "\n",
    "print(f\"✓ Saved to: {output_path}\")\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"Summary by source:\")\n",
    "print(combined_df['SOURCE_DATASET'].value_counts().sort_index())\n",
    "print(f\"{'='*60}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9d1c376c-4443-40cc-81c0-58f0a1eb8790",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'combined_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Aggregate statistics by source dataset\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m agg_stats \u001b[38;5;241m=\u001b[39m \u001b[43mcombined_df\u001b[49m\u001b[38;5;241m.\u001b[39mgroupby(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSOURCE_DATASET\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39magg({\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mID\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcount\u001b[39m\u001b[38;5;124m'\u001b[39m,  \u001b[38;5;66;03m# Number of records\u001b[39;00m\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTITLE_MATCH_COUNT\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msum\u001b[39m\u001b[38;5;124m'\u001b[39m,  \u001b[38;5;66;03m# Sum of title matches\u001b[39;00m\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRETENTIONS_APPLIED\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msum\u001b[39m\u001b[38;5;124m'\u001b[39m  \u001b[38;5;66;03m# Sum of retentions applied\u001b[39;00m\n\u001b[1;32m     11\u001b[0m })\u001b[38;5;241m.\u001b[39mreset_index()\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# Rename columns for clarity\u001b[39;00m\n\u001b[1;32m     14\u001b[0m agg_stats\u001b[38;5;241m.\u001b[39mcolumns \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSource Dataset\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRecord Count\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTotal Title Matches\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTotal Retentions Applied\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'combined_df' is not defined"
     ]
    }
   ],
   "source": [
    "# Visualize aggregate statistics by source dataset\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Aggregate statistics by source dataset\n",
    "agg_stats = combined_df.groupby('SOURCE_DATASET').agg({\n",
    "    'ID': 'count',  # Number of records\n",
    "    'TITLE_MATCH_COUNT': 'sum',  # Sum of title matches\n",
    "    'RETENTIONS_APPLIED': 'sum'  # Sum of retentions applied\n",
    "}).reset_index()\n",
    "\n",
    "# Rename columns for clarity\n",
    "agg_stats.columns = ['Source Dataset', 'Record Count', 'Total Title Matches', 'Total Retentions Applied']\n",
    "\n",
    "print(\"Aggregate Statistics by Source Dataset:\")\n",
    "print(\"=\"*80)\n",
    "print(agg_stats.to_string(index=False))\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Create visualization\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "fig.suptitle('Aggregate Statistics by Source Dataset', fontsize=16, fontweight='bold')\n",
    "\n",
    "# Color palette\n",
    "colors = plt.cm.Set3(np.linspace(0, 1, len(agg_stats)))\n",
    "\n",
    "# Chart 1: Record Count\n",
    "ax1 = axes[0]\n",
    "bars1 = ax1.bar(agg_stats['Source Dataset'], agg_stats['Record Count'], color=colors)\n",
    "ax1.set_xlabel('Source Dataset', fontweight='bold')\n",
    "ax1.set_ylabel('Number of Records', fontweight='bold')\n",
    "ax1.set_title('Record Count per Dataset')\n",
    "ax1.tick_params(axis='x', rotation=45)\n",
    "ax1.grid(axis='y', alpha=0.3)\n",
    "# Add value labels on bars\n",
    "for bar in bars1:\n",
    "    height = bar.get_height()\n",
    "    ax1.text(bar.get_x() + bar.get_width()/2., height,\n",
    "             f'{int(height):,}',\n",
    "             ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "# Chart 2: Total Title Matches\n",
    "ax2 = axes[1]\n",
    "bars2 = ax2.bar(agg_stats['Source Dataset'], agg_stats['Total Title Matches'], color=colors)\n",
    "ax2.set_xlabel('Source Dataset', fontweight='bold')\n",
    "ax2.set_ylabel('Sum of Title Match Count', fontweight='bold')\n",
    "ax2.set_title('Total Title Matches per Dataset')\n",
    "ax2.tick_params(axis='x', rotation=45)\n",
    "ax2.grid(axis='y', alpha=0.3)\n",
    "# Add value labels on bars\n",
    "for bar in bars2:\n",
    "    height = bar.get_height()\n",
    "    ax2.text(bar.get_x() + bar.get_width()/2., height,\n",
    "             f'{int(height):,}',\n",
    "             ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "# Chart 3: Total Retentions Applied\n",
    "ax3 = axes[2]\n",
    "bars3 = ax3.bar(agg_stats['Source Dataset'], agg_stats['Total Retentions Applied'], color=colors)\n",
    "ax3.set_xlabel('Source Dataset', fontweight='bold')\n",
    "ax3.set_ylabel('Sum of Retentions Applied', fontweight='bold')\n",
    "ax3.set_title('Total Retentions Applied per Dataset')\n",
    "ax3.tick_params(axis='x', rotation=45)\n",
    "ax3.grid(axis='y', alpha=0.3)\n",
    "# Add value labels on bars\n",
    "for bar in bars3:\n",
    "    height = bar.get_height()\n",
    "    ax3.text(bar.get_x() + bar.get_width()/2., height,\n",
    "             f'{int(height):,}',\n",
    "             ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Optional: Create a single grouped bar chart as an alternative view\n",
    "fig2, ax = plt.subplots(figsize=(14, 8))\n",
    "\n",
    "# Prepare data for grouped bar chart\n",
    "x = np.arange(len(agg_stats))\n",
    "width = 0.25\n",
    "\n",
    "# Normalize the values for better comparison (each metric on its own scale)\n",
    "record_count_norm = agg_stats['Record Count']\n",
    "title_matches_norm = agg_stats['Total Title Matches']\n",
    "retentions_norm = agg_stats['Total Retentions Applied']\n",
    "\n",
    "# Create bars\n",
    "bars1 = ax.bar(x - width, record_count_norm, width, label='Record Count', alpha=0.8)\n",
    "bars2 = ax.bar(x, title_matches_norm, width, label='Total Title Matches', alpha=0.8)\n",
    "bars3 = ax.bar(x + width, retentions_norm, width, label='Total Retentions Applied', alpha=0.8)\n",
    "\n",
    "# Customize chart\n",
    "ax.set_xlabel('Source Dataset', fontweight='bold', fontsize=12)\n",
    "ax.set_ylabel('Count', fontweight='bold', fontsize=12)\n",
    "ax.set_title('Aggregate Statistics by Source Dataset (Grouped View)', fontsize=14, fontweight='bold')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(agg_stats['Source Dataset'], rotation=45, ha='right')\n",
    "ax.legend(loc='upper right')\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9470a371-e964-4abb-b697-9efc3fa9c904",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
